---
title: "Logistic Regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Variable considered for product distribution	

FEATURES      | DESCRIPTION                                   | POSSIBLE ANSWERS   
------------- | --------------------------------------------- | --------------------
CLIENTE       | client code                                   | [code]           
STATUS        | type of points                                | [turismo; outros]
RENDA         | income                                        | [number]  
ESCOL         | education                                     | [5 to 14]
RESID         | type of residence                             | [1 to 3]
SEXO          | gender                                        | [0 and 1]

Enabling libraries and packages: <br>
```{r pacotes, message=FALSE, warning=FALSE}
# install.packages("caret") 
# install.packages("dplyr") 
# install.packages("ggplot2") 
# install.packages("grid") 
# install.packages("gridExtra") 
# install.packages("gtools") # necessary to run gmodels
# install.packages("gmodels") 
# install.packages("magrittr") 
# install.packages("partykit") 
# install.packages("rattle")
# install.packages("RColorBrewer")
# install.packages("ODBC")
# install.packages("rpart")
# install.packages("rpart.plot")
# install.packages("UsingR")
# install.packages("readxl")

library(caret) 
library(dplyr)
library(ggplot2)
library(grid)
library(gridExtra)
library(gtools) # necessary to run gmodels
library(gmodels)
library(magrittr)
library(partykit)
library(rattle)
library(RColorBrewer) 
library(RODBC)
library(rpart) 
library(rpart.plot)
library(UsingR)
library(readxl)
```

Load dataset punto.xlsx anb visualize structure:
```{r}
punto <- read_excel("E:/Google Drive/Fgv/Modelagem Preditiva - Abraham Laredo/R/Punto.xlsx")
str(punto)
# preserve original Punto datase; create PP
pp=punto
```

Define variable is not numeric but factor:
```{r}
pp$RESID=as.factor(pp$RESID)
pp$SEXO=as.factor(pp$SEXO)
```

A more elegant table format; analysing variable RESID
```{r}
m = table(pp$RESID, pp$STATUS) #pre analyse results
mp = prop.table(m,1)
m;mp
# feature 3 has a higher concentrarion in "tourism"
# we could merge feature 1 and 2 because percentages are close
# some authrs say we cannot work with categories with 5% or less - not true
# 200-300 is reasonable
# advantage of merging features you have less variable and dummies to handle
# logistic regression btw. 0.7087379 and 0.7128028 - in neural networks we try to reduce
# group coefficients 1 and 2 should be similar
```

Same assessment as above: analysing variable SEX
```{r}
m = table(pp$SEXO, pp$STATUS) #pre analise dos resultados
mp = prop.table(m,1)
m;mp
# difference is also not that big: .71 e .73 are very close to each other
# what is big? depends on analyst
# if sample is too small (10 guys), the feature is not worth it (too small) 
# discretize (ranges) to work better
```

<br>
Let's analyse income and discretize in order to analyse "linearity"
```{r}
# install.packages("arules")
library(arules)
#k refers to category
#divide variable krenda in 5 intervals - (200 ppl per inteval in a 1000 ppl sample)
pp$krenda=discretize(pp$RENDA, method = "frequency", categories =5) #using counting type = frequency
```

br>
Same assessment as above: analysing variable RENDA
```{r}
m = table(pp$krenda, pp$STATUS) #pre-assessing results
mp = prop.table(m,1)
m;mp
#sum of outros + turismo close o 200 - variation of +1 is due to ties 
#seeting turismo column, more income you have, your preference towards tourism increases
#use discretized feature/variable
```


br>
Same assessment as above: analysing variable ESCOL
```{r}
m = table(pp$ESCOL,pp$STATUS) #pre analise dos resultados
mp = prop.table(m,1)
m;mp
# linearity problem - using escolatirity ranges
```

br>
Running Logistic Regression
```{r}
# feature crated with binomial status
pp$alvo=ifelse(pp$STATUS=="turismo",1,0)

# user carte to separate sample  


# manual method in case caret does not work
# first. define seed
set.seed(1934)
# user 1000 observations; use 500, with no repetitions
flag=sample(1:1000,500,replace=FALSE) 
ppl = pp[flag,] #generate learning sample
ppt = pp[-flag,] #generate test sample

# glm - linear model - no excel
# run glm with learning sample ppl
fit=glm(data = ppl, alvo~RENDA+ESCOL+RESID+SEXO, family = binomial())

# second. analyse results
summary(fit)

# montar a equacao de Z
# observar a coluna estimate z = -12.30 + 0.0051*Renda + 0.52*Escol...
  # z eh uma funcao linear utilizada para calcular a probabilidade
# encontrou um z de 3.2 por exemplo
# aplicar na formula de P(Tur) = 1 / (1+ exp^-3.2)
# os dados que nao foram incluidos fazem parte do intercep
# o # no estimate nao indica importancia - a escala do valor x pode variar - numero de renda eh muito superior a escolaridade (que vai de 1 a 14)
# eventualmente se vc remover um dos itens menos relevantes, as demais estimativas serao recalibradas
# **** indice akaiki - quantidade de variaveis vs. qualidade preditiva ***
# para selecionar as variaveis
# gerar novo modelo

fit2=step(fit)
summary(fit2)

# testar o modelo - vamos testar as probabilidades
# para descobrir que o computador fez um bom trabalho
# estamos calculando a probabilidade de turismo
# vou analisar o modelo utilizando a amostra teste
# calcular a probabilidade de turismo para cada individuo da amostra teste
# probabilidade de turismo

ppt$ptur = predict(fit2, newdata=ppt, type = "response") # na arvore usamos response
#verificar ajuste do modelo aos dados - modelo calibrado
#quando amostra eh pequena, fazer crossvalidation
#temos probabilidade ptur

# mudar o ponto de corte para 0.7
ppt$klas=ifelse(ppt$ptur>.7,"tur_hat","out_hat")
m=table(ppt$klas,ppt$STATUS)
m
# verificar a taxa de erro global = (44 + 69) / 500
# considerar os custos - classificar os custo - calcular o cara errado de turismo ou outro

# trabalhar com faixas
# ha um limite como trabalhar diferente faixas
# para achar o KS

## install.packages("hmeasure")
library(hmeasure)
HMeasure(ppt$STATUS,ppt$ptur)$metrics
# ks = 0.5095
# mudar o ponto de corte para 0.7

ppt$klas=ifelse(ppt$ptur>.7,"tur_hat","out_hat")
m=table(ppt$klas,ppt$STATUS)
m
# verificar a taxa de erro global = (44 + 60) / 500
# considerar os custos - classificar os custo - calcular o cara errado de turismo ou outro
# nao se usa ponto em modelagem preditiva
  # o que se usa hoje? como ter a balanca

#install.packages("pROC")
library(pROC)
xx=roc(ppt$STATUS,ppt$ptur)
plot(xx)
# over 0.9
# tendency to use h
# whoever created h, has no idea what is good h or bad h


```

